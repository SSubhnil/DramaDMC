BasicSettings:
  ImageSize: 64
  ImageChannel: 3
  ActionSpaceType: continuous
  ReplayBufferOnGPU: True
  Seed: 3710
  Env_name: dm_control/cartpole-balance #ALE/Pong-v5 # dm_control/cartpole-balance
  Device: cuda:0
  Use_amp: True
  Use_cg: True
  Compile: True
  SavePath: None

Evaluate:
   EpisodeNum: 10
   NumEnvs: 4
   DuringTraining: True
   EverySteps: 1000

JointTrainAgent:
  SampleMaxSteps: 105000 # Just to make sure the last episode will finish, no training after 100k
  BufferMaxLength: 100000
  WorldModelWarmUp: 1032
  BehaviourWarmUp: 1032
  NumEnvs: 1
  BatchSize: 16
  BatchLength: 128
  ImagineBatchSize: 512
  ImagineContextLength: 8
  ImagineBatchLength: 16
  RealityContextLength: 16
  TrainDynamicsEverySteps: 1
  TrainDynamicsEpoch: 1
  TrainAgentEverySteps: 1
  FreezeWorldModelAfterSteps: 100000
  FreezeBehaviourAfterSteps: 100000
  SaveEverySteps: 2000
  SaveModels: True
  Tau: 10
  ImaginationTau: 10
  Alpha: 1.0 # High focus on penalising high imagine counts regardless of train counts, less probability to be sampled
  Beta: 1.0 # High focus on penalising 

Models:
  WorldModel:
    dtype: float32
    Backbone: Mamba2 # Mamba, Mamba2, Transformer
    InChannels: 3
    Act: SiLU
    CategoricalDim: 32
    ClassDim: 32
    HiddenStateDim: 512
    Optimiser: Laprop
    LatentDiscreteType: naive
    Max_grad_norm: 1000
    Warmup_steps: 1000
    Dropout: 0.1
    Unimix_ratio: 0.01
    Weight_decay: 1.0e-4
    Adam:
      LearningRate: 1.0e-4
    Laprop:
      LearningRate: 4.0e-5
      Epsilon: 1.0e-20
    Encoder:
      Depth: 16
      Mults:  [1, 2, 3, 4, 4]
      Norm: rms
      Kernel: 5
      Padding: same
      InputSize: [3, 64, 64]
    Decoder:
      Depth: 16
      Mults:  [1, 2, 3, 4, 4]
      Norm: rms
      Kernel: 5
      Padding: same
      FirstStrideOne: True
      InputSize: [3, 64, 64]
      FinalLayerSigmoid: True
    Reward:
      HiddenUnits: 256
      LayerNum: 1
    Termination:
      HiddenUnits: 256
      LayerNum: 1      
    Transformer:
      FinalFeatureWidth: 4
      NumLayers: 2
      NumHeads: 8
    Mamba:
      n_layer: 2
      d_intermediate: 0
      ssm_cfg:
        d_state: 16
    

  Agent:
    dtype: float32
    Policy: AC # AC or PPO
    Unimix_ratio: 0
    AC:
      NumLayers: 3    
      Gamma: 0.985
      Lambda: 0.95
      EntropyCoef: 3.e-4
      Max_grad_norm: 100
      Warmup_steps: 1000
      Act: SiLU
      Optimiser: Laprop
      Adam:
        LearningRate: 3.0e-5
        Epsilon: 1.0e-5
      Laprop:
        LearningRate: 4.0e-5
        Epsilon: 1.0e-20
      Actor:
        HiddenUnits: 256
      Critic:
        HiddenUnits: 512  
    PPO:
      NumLayers: 3
      Gamma: 0.985
      Lambda: 0.95
      EpsilonClip: 0.2
      K_epochs: 3
      Minibatch: 16384
      CriticCoef: 1
      EntropyCoef: 3.e-4
      KL_threshold: 0.01
      Max_grad_norm: 100
      Warmup_steps: 1000
      Act: SiLU
      Optimiser: Laprop
      Adam:
        LearningRate: 3.e-5
        Epsilon: 1.0e-5
      Laprop:
        LearningRate: 4.0e-5
        Epsilon: 1.0e-20
      Actor:
        HiddenUnits: 256
      Critic:
        HiddenUnits: 512 # Andrychowicz2020 wider critic network seems better  
Wandb:
  Init:
    Mode: disabled
    Project: Mamba_dreamer
n: standard